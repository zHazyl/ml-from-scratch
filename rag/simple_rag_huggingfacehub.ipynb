{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "606986be-fd43-4b0f-b69b-02250e57e4b0",
      "metadata": {
        "id": "606986be-fd43-4b0f-b69b-02250e57e4b0"
      },
      "source": [
        "### Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5",
      "metadata": {
        "tags": [],
        "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch datasets tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
        "!pip install -q peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7\n",
        "!pip install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -U git+https://github.com/huggingface/accelerate.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  BitsAndBytesConfig,\n",
        "  pipeline\n",
        ")\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "import nest_asyncio\n",
        "import transformers"
      ],
      "metadata": {
        "id": "XUU4oWOxO98T"
      },
      "id": "XUU4oWOxO98T",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "# Tokenizer\n",
        "#################################################################\n",
        "\n",
        "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "tIZW1r0SJ0bz"
      },
      "id": "tIZW1r0SJ0bz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "model = HuggingFaceHub(repo_id=model_name, model_kwargs={\n",
        "    # \"tokenizer\":tokenizer,\n",
        "    \"temperature\":0.01,\"repetition_penalty\":1.1,\"return_full_text\":True,\"max_new_tokens\":1000})"
      ],
      "metadata": {
        "id": "jm_f7CEB_boi",
        "outputId": "6a91e262-9111-4c04-d30f-a304959ab668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jm_f7CEB_boi",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!npx playwright install"
      ],
      "metadata": {
        "id": "awSOdIww8Zt1"
      },
      "id": "awSOdIww8Zt1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "articles = [\"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
        "            \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
        "            \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
        "            \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
        "            \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\"]\n",
        "\n",
        "# Scrapes the blogs above\n",
        "loader = AsyncChromiumLoader(articles)\n",
        "docs = loader.load()\n",
        "\n",
        "# Converts HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100,\n",
        "                                      chunk_overlap=0)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "db = FAISS.from_documents(chunked_documents,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "\n",
        "# Connect query to FAISS index using a retriever\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 4}\n",
        ")"
      ],
      "metadata": {
        "id": "LLwoto36eWvj"
      },
      "id": "LLwoto36eWvj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did Laporta say?\"\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "P26aFA-dgNvc",
        "outputId": "da27a8b2-292c-402c-8291-34e9f46f2389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P26aFA-dgNvc",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". I tapped LaPorta due to a more favorable matchup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "### [INST]\n",
        "Instruction: Answer the question based on your\n",
        "fantasy football knowledge. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        "\n",
        "[/INST]\n",
        " \"\"\"\n",
        "\n",
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=model, prompt=prompt)"
      ],
      "metadata": {
        "id": "S4KxCg1dgt_W"
      },
      "id": "S4KxCg1dgt_W",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"context\":\"\",\n",
        "                  \"question\": \"Should I pick up Alvin Kamara for my fantasy team?\"})"
      ],
      "metadata": {
        "id": "Y3akShQBg35-",
        "outputId": "c09a7d82-b92d-4f8a-9a37-d1c5c260768f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Y3akShQBg35-",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': '',\n",
              " 'question': 'Should I pick up Alvin Kamara for my fantasy team?',\n",
              " 'text': '\\n### [INST]\\nInstruction: Answer the question based on your\\nfantasy football knowledge. Here is context to help:\\n\\n\\n\\n### QUESTION:\\nShould I pick up Alvin Kamara for my fantasy team?\\n\\n[/INST]\\n \\nBased on your fantasy football knowledge, it depends on several factors such as the specific league rules, roster requirements, and the current performance of Alvin Kamara. However, in general, Alvin Kamara can be a valuable addition to any fantasy football team due to his versatility and potential for high production. He is a dual-threat running back who can also catch passes, making him a valuable asset in PPR (Punt Returner) leagues. Additionally, he has shown consistency in his performance over the past few seasons, with multiple top-24 finishes in standard leagues. Therefore, if you have an open spot on your roster and are looking for a reliable player to add depth to your offense, Alvin Kamara could be a good option to consider.'}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standalone_query_generation_llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\n",
        "    \"temperature\":0.01,\"repetition_penalty\":1.1,\"return_full_text\":True,\"max_new_tokens\":1000})\n",
        "\n",
        "response_generation_llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\n",
        "    \"temperature\":0.01,\"repetition_penalty\":1.1,\"return_full_text\":True,\"max_new_tokens\":1000})"
      ],
      "metadata": {
        "id": "oGPSz0XB-5tz"
      },
      "id": "oGPSz0XB-5tz",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "_template = \"\"\"\n",
        "[INST]\n",
        "Given the following conversation and a follow up question,\n",
        "rephrase the follow up question to be a standalone question, in its original language,\n",
        "that can be used to query a FAISS index. This query will be used to retrieve documents with additional context.\n",
        "\n",
        "Let me share a couple examples.\n",
        "\n",
        "If you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\n",
        "```\n",
        "Chat History:\n",
        "Follow Up Input: How is Lawrence doing?\n",
        "Standalone Question:\n",
        "How is Lawrence doing?\n",
        "```\n",
        "\n",
        "If this is the second question onwards, you should properly rephrase the question like this:\n",
        "```\n",
        "Chat History:\n",
        "Human: How is Lawrence doing?\n",
        "AI:\n",
        "Lawrence is injured and out for the season.\n",
        "Follow Up Input: What was his injury?\n",
        "Standalone Question:\n",
        "What was Lawrence's injury?\n",
        "```\n",
        "\n",
        "Now, with those examples, here is the actual chat history and input question.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\n",
        "[your response here]\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "STANDALONE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
      ],
      "metadata": {
        "id": "Htm-GGYf3UUw"
      },
      "id": "Htm-GGYf3UUw",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import format_document\n",
        "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "# Instantiate ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        " return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")\n",
        "# First, load the memory to access chat history\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        " chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "# Define the standalone_question step to process the question and chat history\n",
        "standalone_question = {\n",
        " \"standalone_question\": {\n",
        " \"question\": lambda x: x[\"question\"],\n",
        " \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        " }\n",
        " | STANDALONE_QUESTION_PROMPT,\n",
        "}\n",
        "# Finally, output the result of the CONDENSE_QUESTION_PROMPT\n",
        "output_prompt = {\n",
        " \"standalone_question_prompt_result\": itemgetter(\"standalone_question\"),\n",
        "}\n",
        "# Combine the steps into a final chain\n",
        "standalone_query_generation_prompt = loaded_memory | standalone_question | output_prompt"
      ],
      "metadata": {
        "id": "BeimRBy93jsg"
      },
      "id": "BeimRBy93jsg",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Instantiate ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        " return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")\n",
        "# First, load the memory to access chat history\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        " chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")"
      ],
      "metadata": {
        "id": "PFQsgdsH4FUv"
      },
      "id": "PFQsgdsH4FUv",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"how is mahomes doing?\"}\n",
        "memory.save_context(inputs, {\"answer\": \"mahomes is not looking great! bench him!\"})"
      ],
      "metadata": {
        "id": "2ZV6HrR84I5j"
      },
      "id": "2ZV6HrR84I5j",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"who should I replace him with?\"}\n",
        "standalone_query_generation_prompt.invoke(inputs)['standalone_question_prompt_result']"
      ],
      "metadata": {
        "id": "MSdb0F7A4PgP",
        "outputId": "8b31699f-e024-4a07-e42f-7199fdd977ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MSdb0F7A4PgP",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='\\n[INST]\\nGiven the following conversation and a follow up question,\\nrephrase the follow up question to be a standalone question, in its original language,\\nthat can be used to query a FAISS index. This query will be used to retrieve documents with additional context.\\n\\nLet me share a couple examples.\\n\\nIf you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\\n```\\nChat History:\\nFollow Up Input: How is Lawrence doing?\\nStandalone Question:\\nHow is Lawrence doing?\\n```\\n\\nIf this is the second question onwards, you should properly rephrase the question like this:\\n```\\nChat History:\\nHuman: How is Lawrence doing?\\nAI:\\nLawrence is injured and out for the season.\\nFollow Up Input: What was his injury?\\nStandalone Question:\\nWhat was Lawrence\\'s injury?\\n```\\n\\nNow, with those examples, here is the actual chat history and input question.\\nChat History:\\n\\nFollow Up Input: who should I replace him with?\\nStandalone question:\\n[your response here]\\n[/INST]\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def runnable():\n",
        "  standalone_query_generation_chain = (\n",
        "    loaded_memory\n",
        "    | {\n",
        "    \"question\": lambda x: x[\"question\"],\n",
        "    \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        "    }\n",
        "    | STANDALONE_QUESTION_PROMPT\n",
        "    | standalone_query_generation_llm\n",
        "  )\n",
        "  return standalone_query_generation_chain\n",
        "\n",
        "inputs = {\"question\": \"who should I replace him with?\"}\n",
        "runnable().invoke(inputs)"
      ],
      "metadata": {
        "id": "Ur31bMDF4xE3",
        "outputId": "b88e8fea-8c62-482f-92c3-ec6a861a2486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "id": "Ur31bMDF4xE3",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n[INST]\\nGiven the following conversation and a follow up question,\\nrephrase the follow up question to be a standalone question, in its original language,\\nthat can be used to query a FAISS index. This query will be used to retrieve documents with additional context.\\n\\nLet me share a couple examples.\\n\\nIf you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\\n```\\nChat History:\\nFollow Up Input: How is Lawrence doing?\\nStandalone Question:\\nHow is Lawrence doing?\\n```\\n\\nIf this is the second question onwards, you should properly rephrase the question like this:\\n```\\nChat History:\\nHuman: How is Lawrence doing?\\nAI:\\nLawrence is injured and out for the season.\\nFollow Up Input: What was his injury?\\nStandalone Question:\\nWhat was Lawrence\\'s injury?\\n```\\n\\nNow, with those examples, here is the actual chat history and input question.\\nChat History:\\nHuman: how is mahomes doing?\\nAI: mahomes is not looking great! bench him!\\nFollow Up Input: who should I replace him with?\\nStandalone question:\\n[your response here]\\n[/INST]\\nStandalone Question: Who should I replace Mahomes with?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "[INST]\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {standalone_question}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "RESPONSE_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "def _combine_documents(\n",
        " docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        " doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        " return document_separator.join(doc_strings)\n",
        "\n",
        "# First we add a step to load memory\n",
        "# This adds a \"memory\" key to the input object\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        " chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "\n",
        "# Now we calculate the standalone question\n",
        "standalone_question = {\n",
        "    \"standalone_question\": {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        "    }\n",
        "    | STANDALONE_QUESTION_PROMPT\n",
        "    | standalone_query_generation_llm,\n",
        "}\n",
        "# Now we retrieve the documents\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"standalone_question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "# Now we construct the inputs for the final prompt\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"standalone_question\": itemgetter(\"standalone_question\"),\n",
        "}\n",
        "\n",
        "# And finally, we do the part that returns the answers\n",
        "answer = {\n",
        "    \"answer\": final_inputs | RESPONSE_PROMPT | response_generation_llm,\n",
        "    \"standalone_question\": itemgetter(\"standalone_question\"),\n",
        "    \"context\": final_inputs[\"context\"]\n",
        "}\n",
        "# And now we put it all together!\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
      ],
      "metadata": {
        "id": "fsnHa80F6dPA"
      },
      "id": "fsnHa80F6dPA",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"How is Mahomes doing?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ],
      "metadata": {
        "id": "2yC79QNt_tIa",
        "outputId": "30861a6e-ea39-47f5-ee40-37242ef5b741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2yC79QNt_tIa",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': '\\n[INST]\\nAnswer the question based only on the following context:\\nCurrent Article\\n\\n|\\n\\n__2 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read\\n\\nQuestion: \\n[INST]\\nGiven the following conversation and a follow up question,\\nrephrase the follow up question to be a standalone question, in its original language,\\nthat can be used to query a FAISS index. This query will be used to retrieve documents with additional context.\\n\\nLet me share a couple examples.\\n\\nIf you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\\n```\\nChat History:\\nFollow Up Input: How is Lawrence doing?\\nStandalone Question:\\nHow is Lawrence doing?\\n```\\n\\nIf this is the second question onwards, you should properly rephrase the question like this:\\n```\\nChat History:\\nHuman: How is Lawrence doing?\\nAI:\\nLawrence is injured and out for the season.\\nFollow Up Input: What was his injury?\\nStandalone Question:\\nWhat was Lawrence\\'s injury?\\n```\\n\\nNow, with those examples, here is the actual chat history and input question.\\nChat History:\\n\\nFollow Up Input: How is Mahomes doing?\\nStandalone question:\\n[your response here]\\n[/INST]\\nStandalone Question:\\nWhat is Mahomes\\' current health status?\\n[/INST]\\nNote: The standalone question assumes that the user is asking about Mahomes\\' current health status, rather than just asking how he is doing.',\n",
              " 'standalone_question': '\\n[INST]\\nGiven the following conversation and a follow up question,\\nrephrase the follow up question to be a standalone question, in its original language,\\nthat can be used to query a FAISS index. This query will be used to retrieve documents with additional context.\\n\\nLet me share a couple examples.\\n\\nIf you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\\n```\\nChat History:\\nFollow Up Input: How is Lawrence doing?\\nStandalone Question:\\nHow is Lawrence doing?\\n```\\n\\nIf this is the second question onwards, you should properly rephrase the question like this:\\n```\\nChat History:\\nHuman: How is Lawrence doing?\\nAI:\\nLawrence is injured and out for the season.\\nFollow Up Input: What was his injury?\\nStandalone Question:\\nWhat was Lawrence\\'s injury?\\n```\\n\\nNow, with those examples, here is the actual chat history and input question.\\nChat History:\\n\\nFollow Up Input: How is Mahomes doing?\\nStandalone question:\\n[your response here]\\n[/INST]\\nStandalone Question:\\nWhat is Mahomes\\' current health status?',\n",
              " 'context': 'Current Article\\n\\n|\\n\\n__2 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read'}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save previous question and answer to memory\n",
        "memory.save_context(inputs, {\"answer\": result[\"answer\"]})\n",
        "\n",
        "inputs = {\"question\": \"Who are good alternatives to him right now?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ],
      "metadata": {
        "id": "LlVp87AhAI7Y"
      },
      "id": "LlVp87AhAI7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standalone_query_generation_llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\n",
        "    \"temperature\":0.01,\"repetition_penalty\":1.1,\n",
        "    # \"return_full_text\":True,\n",
        "    \"max_new_tokens\":1000\n",
        "    })\n",
        "\n",
        "response_generation_llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\n",
        "    \"temperature\":0.01,\"repetition_penalty\":1.1,\n",
        "    # \"return_full_text\":True,\n",
        "    \"max_new_tokens\":1000\n",
        "    })\n",
        "# Instantiate ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        " return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        " chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "standalone_question = {\n",
        "    \"standalone_question\": {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        "    }\n",
        "    | STANDALONE_QUESTION_PROMPT\n",
        "    | standalone_query_generation_llm,\n",
        "}\n",
        "# Now we retrieve the documents\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"standalone_question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "# Now we construct the inputs for the final prompt\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"standalone_question\": itemgetter(\"standalone_question\"),\n",
        "}\n",
        "answer = {\n",
        "    \"answer\": final_inputs | RESPONSE_PROMPT | response_generation_llm\n",
        "}\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer\n",
        "inputs = {\"question\": \"Should I start Gibbs next week for fantasy?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ],
      "metadata": {
        "id": "lnIIlSefaQmM",
        "outputId": "7b4a3d7b-a599-442f-c5f6-cec368178a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lnIIlSefaQmM",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': '\\n[INST]\\nAnswer the question based only on the following context:\\nCurrent Article\\n\\n|\\n\\n__2 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read\\n\\nCurrent Article\\n\\n|\\n\\n__3 min read\\n\\nQuestion: \\n[INST]\\nGiven the following conversation and a follow up question,\\nrephrase the follow up question to be a standalone question, in its original language,\\nthat can be used to query a FAISS index. This query will be used to retrieve documents with additional context.\\n\\nLet me share a couple examples.\\n\\nIf you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\\n```\\nChat History:\\nFollow Up Input: How is Lawrence doing?\\nStandalone Question:\\nHow is Lawrence doing?\\n```\\n\\nIf this is the second question onwards, you should properly rephrase the question like this:\\n```\\nChat History:\\nHuman: How is Lawrence doing?\\nAI:\\nLawrence is injured and out for the season.\\nFollow Up Input: What was his injury?\\nStandalone Question:\\nWhat was Lawrence\\'s injury?\\n```\\n\\nNow, with those examples, here is the actual chat history and input question.\\nChat History:\\n\\nFollow Up Input: Should I start Gibbs next week for fantasy?\\nStandalone question:\\n[your response here]\\n[/INST]\\nStandalone Question:\\nShould I start Gibbs next week for fantasy?\\n[/INST]\\nStandalone Question:\\nIs Gibbs a good option for fantasy football next week?'}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\n",
        "    \"temperature\":0.01,\"repetition_penalty\":1.1,\n",
        "    # \"return_full_text\":True,\"max_new_tokens\":1000\n",
        "    })\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "### [INST] Instruction: Answer the question based on your fantasy football knowledge. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question} [/INST]\n",
        " \"\"\"\n",
        "\n",
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)\n",
        "\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"Should I start Gibbs next week for fantasy?\")"
      ],
      "metadata": {
        "id": "Ug8pTcKmeQW2"
      },
      "id": "Ug8pTcKmeQW2",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['text']"
      ],
      "metadata": {
        "id": "T4IXXqt3f7bx",
        "outputId": "3aaf9dc2-608e-41b6-b175-54b48223cded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "id": "T4IXXqt3f7bx",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n### [INST] Instruction: Answer the question based on your fantasy football knowledge. Here is context to help:\\n\\n[Document(page_content='This week, Harris faces the bottom-of-the-barrel Packers’ run defense that\\\\nallows the ninth-most fantasy points per game to the running back position.\\\\nHarris will give you a higher-volume RB with a low rostership percentage this\\\\nweek.', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/'}), Document(page_content='could start cutting into his workload. Furthermore, his rest of the season\\\\nschedule isn’t fantasy-friendly. Try to flip Edwards and a WR3 for Kenneth\\\\nWalker or Tony Pollard', metadata={'source': 'https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/'}), Document(page_content='“**Gus Edwards** has been on fire lately. He is the RB1 over the past three\\\\nweeks, averaging 22.2 half-point PPR fantasy points and two rushing touchdowns\\\\nper game. However, over 54% of his fantasy production came from the six\\\\nrushing touchdowns. Meanwhile, the veteran averaged only 6.5 fantasy points\\\\nper game over the first six contests. He had more than six fantasy points only\\\\nonce, in the Week 2 matchup where Edwards found the end zone. The veteran\\\\nrunning back is a touchdown-or-bust player, and Keaton Mitchell', metadata={'source': 'https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/'}), Document(page_content=', as he should have plenty of field goal opportunities to both miss and make.\\\\nAt running back, I will be going with CMC and Alvin Kamara', metadata={'source': 'https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/'})]\\n\\n### QUESTION:\\nShould I start Gibbs next week for fantasy? [/INST]\\n \\nBased on the information provided, it seems like Gus Edwards has been performing well recently and could be a good option for fantasy football next week. However, it's important to note that his production is heavily dependent on rushing touchdowns, which can be unpredictable. Additionally, his schedule for the rest of the season may not be very favorable for fantasy players. It might be worth considering other options such as Christian McCaffrey (CMC) or Alvin Kamara, who\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ou6e_ydDh-PM"
      },
      "id": "Ou6e_ydDh-PM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-gpu.m114",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-gpu:m114"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}