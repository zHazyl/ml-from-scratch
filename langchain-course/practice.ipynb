{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs4xSyvjL+wity2LTkenWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zHazyl/ml-from-scratch/blob/main/langchain-course/practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "requirements = \"\"\"\n",
        "langchain_google_genai\n",
        "langchain\n",
        "python-dotenv\n",
        "huggingface-hub\n",
        "openai\n",
        "chromadb\n",
        "pypdf\n",
        "\"\"\"\n",
        "\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "  f.write(requirements)"
      ],
      "metadata": {
        "id": "ZVq8NLPnL0mr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ugOTjbFsspLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# import google.generativeai as genai\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import MessagesPlaceholder, HumanMessagePromptTemplate, ChatPromptTemplate, PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory, ConversationSummaryMemory\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "chat = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    # client=genai,\n",
        "    temperature=0.3,\n",
        "    # convert_system_message_to_human=True\n",
        ")\n",
        "\n",
        "# chat = HuggingFaceHub(repo_id=\"abacusai/Liberated-Qwen1.5-72B\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
        "\n",
        "# chat = ChatOpenAI()\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    chat_memory=FileChatMessageHistory(\"messages.json\"),\n",
        "    memory_key=\"messages\",\n",
        "    return_messages=True)\n",
        "\n",
        "# memory = ConversationSummaryMemory(\n",
        "#     # chat_memory=FileChatMessageHistory(\"messages.json\"),\n",
        "#     memory_key=\"messages\",\n",
        "#     return_messages=True,\n",
        "#     llm=chat\n",
        "# )\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    messages=[\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# memory_prompt_template = \"\"\"<|im_start|>system\n",
        "# You are Liberated, a helpful AI assistant.<|im_end|>\n",
        "# <|im_start|>user\n",
        "# {content}<|im_end|>\n",
        "# \"\"\"\n",
        "\n",
        "# def create_prompt_from_template(template):\n",
        "#     return PromptTemplate.from_template(template)\n",
        "\n",
        "# prompt = create_prompt_from_template(memory_prompt_template)\n",
        "\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=chat,\n",
        "    prompt=prompt,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "while True:\n",
        "\n",
        "    content = input(\">> \")\n",
        "\n",
        "    result = chain({\"content\": content})\n",
        "\n",
        "    print(result['text'])\n"
      ],
      "metadata": {
        "id": "wesswBwesySG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import BaseRetriever\n",
        "\n",
        "class RedundantFilterRetriever(BaseRetriever):\n",
        "  embeddings: Embeddings\n",
        "  chroma: Chroma\n",
        "\n",
        "  def get_relevant_documents(self, query):\n",
        "    emb = self.embeddings.embed_query(query)\n",
        "    return self.chroma.max_marginal_relevance_search_by_vector(\n",
        "        embedding=emb,\n",
        "        lambda_mult=0.8\n",
        "    )\n",
        "\n",
        "  async def aget_relevant_documents(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "p-njsAW4tiwB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "response_model = GoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    temperature=0.2,\n",
        "    task=\"text-generation\",\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True\n",
        ")"
      ],
      "metadata": {
        "id": "DcyRkuVqatKe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standalone_model = GoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    temperature=0,\n",
        "    task=\"text-generation\",\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True\n",
        ")"
      ],
      "metadata": {
        "id": "jGWOMhnoe3Po"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_model.invoke(\"System: you are my boss. Me: What's up. You:\")"
      ],
      "metadata": {
        "id": "UwxVXriXbKMN",
        "outputId": "a081c886-b239-4791-bab2-85469a2b04ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I need you to work on a project for me.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "db = Chroma(\n",
        "    persist_directory=\"emb\",\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "retriever = RedundantFilterRetriever(\n",
        "    embeddings=embeddings,\n",
        "    chroma=db\n",
        ")"
      ],
      "metadata": {
        "id": "MHIZCp_GQ8a3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "id": "2Rk5x-c3dYn9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load_and_split()"
      ],
      "metadata": {
        "id": "x_fRzxHAf4Z7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\\n\".join(str(p.page_content) for p in data)"
      ],
      "metadata": {
        "id": "wXRrrYcwgVum"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "EvOZC2Sig4Ko"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_text(context)"
      ],
      "metadata": {
        "id": "6xP2vFqkg51_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.add_texts(texts)"
      ],
      "metadata": {
        "id": "K-5RwIZ8hozF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"What is Spring?\")"
      ],
      "metadata": {
        "id": "Ifc2XOJ1nEue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "custom_prompt_template = \"\"\"\n",
        "### System:\n",
        "You are an AI assistant that follows instructions extremely well. Help as much as you can.\n",
        "### User:\n",
        "You are a research assistant for an artificial intelligence student. Use only the following information to answer user queries:\n",
        "Context= {context}\n",
        "History = {history}\n",
        "Question= {question}\n",
        "### Assistant:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                        input_variables=[\"question\", \"context\", \"history\"])"
      ],
      "metadata": {
        "id": "sOOAnMvCnPN8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(input_key=\"question\",\n",
        "                                   memory_key=\"history\",\n",
        "                                   return_messages=True)"
      ],
      "metadata": {
        "id": "liaF4KPcpSJY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "response_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    temperature=0.2,\n",
        "    task=\"text-generation\",\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True\n",
        ")"
      ],
      "metadata": {
        "id": "dXztC_WErS3E"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from operator import itemgetter\n",
        "\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        " chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")"
      ],
      "metadata": {
        "id": "_zdXkZsYxYtd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_core.messages import get_buffer_string\n",
        "\n",
        "_template = \"\"\"\n",
        "Given the following conversation and a follow up question,\n",
        "rephrase the follow up question to be a standalone question, in English,\n",
        "that can be used to query a FAISS index. This query will be used to retrieve documents with additional context.\n",
        "\n",
        "Let me share a couple examples.\n",
        "\n",
        "If you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\n",
        "```\n",
        "Chat History:\n",
        "Follow Up Input: How is Lawrence doing?\n",
        "Standalone Question:\n",
        "How is Lawrence doing?\n",
        "```\n",
        "\n",
        "If this is the second question onwards, you should properly rephrase the question like this:\n",
        "```\n",
        "Chat History:\n",
        "Human: How is Lawrence doing?\n",
        "AI:\n",
        "Lawrence is injured and out for the season.\n",
        "Follow Up Input: What was his injury?\n",
        "Standalone Question:\n",
        "What was Lawrence's injury?\n",
        "```\n",
        "\n",
        "Now, with those examples, here is the actual chat history and input question.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\n",
        "[your response here]\n",
        "\"\"\"\n",
        "\n",
        "STANDALONE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "standalone_question = {\n",
        "    \"standalone_question\": {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        "    }\n",
        "    | STANDALONE_QUESTION_PROMPT\n",
        "    | standalone_model,\n",
        "}"
      ],
      "metadata": {
        "id": "421x25pqvxaB"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "                      llm=response_model,\n",
        "                      chain_type='map_rerank',\n",
        "                      retriever = retriever,\n",
        "                      return_source_documents = True,\n",
        "                      chain_type_kwargs = {\n",
        "                          # \"verbose\": False,\n",
        "                          # \"prompt\": prompt,\n",
        "                          \"memory\": memory\n",
        "                          }\n",
        "                      )"
      ],
      "metadata": {
        "id": "OFRkMtXspUyn"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Spring?\"\n",
        "response = qa_chain({\"query\": query})\n",
        "response['result']"
      ],
      "metadata": {
        "id": "AqvPXAfeutzj",
        "outputId": "83227783-a7d7-4b58-c0af-7778512c21cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Spring is a framework for developing Java applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is it purpose?\"\n",
        "response = qa_chain({\"query\": query})\n",
        "response['result']"
      ],
      "metadata": {
        "id": "IM-01HJFvEah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain =  loaded_memory | standalone_question | {\"query\": itemgetter(\"standalone_question\")} | qa_chain"
      ],
      "metadata": {
        "id": "_iRy_xIivc0w"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"What is definition of Reactive Streams?\"}\n",
        "final_chain.invoke(inputs)"
      ],
      "metadata": {
        "id": "O_5R7PLG2Joi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"What is it use for\"}\n",
        "final_chain.invoke(inputs)"
      ],
      "metadata": {
        "id": "hah9B_No2WZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q4V3tPsoAqyW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}